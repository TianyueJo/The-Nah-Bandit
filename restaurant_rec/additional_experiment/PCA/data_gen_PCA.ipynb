{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dimension-reduced contexts to d=6/user_data_filtered.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "def kernel_pca_contexts(\n",
    "    input_csv='data.csv',\n",
    "    output_csv='data_kpca.csv',\n",
    "    n_context_columns=30,\n",
    "    dim_out=4\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Reads a CSV that has columns:\n",
    "       - 'num_option' (indicating how many contexts are valid in this row)\n",
    "       - 'context_0', 'context_1', ..., up to 'context_{n_context_columns-1}'\n",
    "         each is either NaN or a string like \"[0.75, 0.75, ...]\"\n",
    "    2. Collects all valid contexts across all rows into one list (N x 9).\n",
    "    3. Applies Kernel PCA to reduce from 9 dims to dim_out.\n",
    "    4. Places the transformed contexts back into the DataFrame.\n",
    "    5. Saves to a new CSV file.\n",
    "    \"\"\"\n",
    "    # 1. Read the CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 2. Prepare to collect all contexts across rows\n",
    "    #    We'll remember which (row, context_index) each vector came from\n",
    "    all_contexts = []       # will be a list of length-9 lists\n",
    "    mapping = []            # will hold (row_index, context_col_index)\n",
    "    \n",
    "    # For each row, read the number of valid contexts (num_option),\n",
    "    # parse them, and append to all_contexts\n",
    "    for row_idx, row in df.iterrows():\n",
    "        num_opt = int(row['num_option']) if not pd.isna(row['num_option']) else 0\n",
    "        \n",
    "        # For each context_i up to num_opt\n",
    "        for c in range(num_opt):\n",
    "            col_name = f\"context_{c}\"\n",
    "            if col_name not in df.columns:\n",
    "                continue  # if the column doesn't exist, skip\n",
    "            \n",
    "            val = row[col_name]\n",
    "            if pd.isna(val):\n",
    "                continue  # no content in this context\n",
    "            val_str = str(val).strip()\n",
    "            if not val_str:\n",
    "                continue  # empty string\n",
    "            # Parse the string (e.g. \"[0.75, 0.75, 0.3333, 1, 0, 0, 0, 0, 0]\") into a list\n",
    "            parsed_list = ast.literal_eval(val_str)  # safer than eval\n",
    "            \n",
    "            # Expecting each context to have exactly 9 floats\n",
    "            if len(parsed_list) != 9:\n",
    "                print(f\"Warning: row={row_idx}, col={col_name} has unexpected length {len(parsed_list)}\")\n",
    "            \n",
    "            all_contexts.append(parsed_list)\n",
    "            mapping.append((row_idx, c))\n",
    "    \n",
    "    # If no contexts found, just save the file as-is or handle differently\n",
    "    if not all_contexts:\n",
    "        print(\"No valid contexts found. Saving original CSV without changes.\")\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        return\n",
    "    \n",
    "    # 3. Convert to a NumPy array (N x 9)\n",
    "    X = np.array(all_contexts, dtype=float)\n",
    "\n",
    "    # 4. Perform Kernel PCA (reduce to dim_out)\n",
    "    kpca = KernelPCA(n_components=dim_out, kernel='rbf')\n",
    "    X_kpca = kpca.fit_transform(X)  # shape: N x dim_out\n",
    "    \n",
    "    # 5. Place the transformed vectors back into the DataFrame\n",
    "    #    We'll store them as a string, e.g. \"[0.123, 0.456, ...]\"\n",
    "    for i, (row_idx, context_idx) in enumerate(mapping):\n",
    "        new_vec = X_kpca[i, :]\n",
    "        df.loc[row_idx, f\"context_{context_idx}\"] = str(new_vec.tolist())\n",
    "    \n",
    "    # 6. Save to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved dimension-reduced contexts to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "d=6\n",
    "kernel_pca_contexts(\n",
    "    input_csv='../data/user_data_filtered.csv',       # Your input CSV\n",
    "    output_csv=f'd={d}/user_data_filtered.csv', # Output CSV with dimension-reduced contexts\n",
    "    n_context_columns=30,       # Maximum number of context columns\n",
    "    dim_out=d                   # Desired output dimension\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
